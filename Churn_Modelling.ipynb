{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Churn Modelling Data Set Analysis Using ANN"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Data Preprocessing"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Part 1 - Data Preprocessing\n\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/matplotlib/font_manager.py:229: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n  'Matplotlib is building the font cache using fc-list. '\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Importing the dataset\ndataset = pd.read_csv('Churn_Modelling.csv')\ndataset.head(10)",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RowNumber</th>\n      <th>CustomerId</th>\n      <th>Surname</th>\n      <th>CreditScore</th>\n      <th>Geography</th>\n      <th>Gender</th>\n      <th>Age</th>\n      <th>Tenure</th>\n      <th>Balance</th>\n      <th>NumOfProducts</th>\n      <th>HasCrCard</th>\n      <th>IsActiveMember</th>\n      <th>EstimatedSalary</th>\n      <th>Exited</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>15634602</td>\n      <td>Hargrave</td>\n      <td>619</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>42</td>\n      <td>2</td>\n      <td>0.00</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>101348.88</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>15647311</td>\n      <td>Hill</td>\n      <td>608</td>\n      <td>Spain</td>\n      <td>Female</td>\n      <td>41</td>\n      <td>1</td>\n      <td>83807.86</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>112542.58</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>15619304</td>\n      <td>Onio</td>\n      <td>502</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>42</td>\n      <td>8</td>\n      <td>159660.80</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113931.57</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>15701354</td>\n      <td>Boni</td>\n      <td>699</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>39</td>\n      <td>1</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>93826.63</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>15737888</td>\n      <td>Mitchell</td>\n      <td>850</td>\n      <td>Spain</td>\n      <td>Female</td>\n      <td>43</td>\n      <td>2</td>\n      <td>125510.82</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>79084.10</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>15574012</td>\n      <td>Chu</td>\n      <td>645</td>\n      <td>Spain</td>\n      <td>Male</td>\n      <td>44</td>\n      <td>8</td>\n      <td>113755.78</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>149756.71</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>15592531</td>\n      <td>Bartlett</td>\n      <td>822</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>50</td>\n      <td>7</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>10062.80</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>15656148</td>\n      <td>Obinna</td>\n      <td>376</td>\n      <td>Germany</td>\n      <td>Female</td>\n      <td>29</td>\n      <td>4</td>\n      <td>115046.74</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>119346.88</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>15792365</td>\n      <td>He</td>\n      <td>501</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>44</td>\n      <td>4</td>\n      <td>142051.07</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>74940.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>15592389</td>\n      <td>H?</td>\n      <td>684</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>27</td>\n      <td>2</td>\n      <td>134603.88</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>71725.73</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n0          1    15634602  Hargrave          619    France  Female   42   \n1          2    15647311      Hill          608     Spain  Female   41   \n2          3    15619304      Onio          502    France  Female   42   \n3          4    15701354      Boni          699    France  Female   39   \n4          5    15737888  Mitchell          850     Spain  Female   43   \n5          6    15574012       Chu          645     Spain    Male   44   \n6          7    15592531  Bartlett          822    France    Male   50   \n7          8    15656148    Obinna          376   Germany  Female   29   \n8          9    15792365        He          501    France    Male   44   \n9         10    15592389        H?          684    France    Male   27   \n\n   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n0       2       0.00              1          1               1   \n1       1   83807.86              1          0               1   \n2       8  159660.80              3          1               0   \n3       1       0.00              2          0               0   \n4       2  125510.82              1          1               1   \n5       8  113755.78              2          1               0   \n6       7       0.00              2          1               1   \n7       4  115046.74              4          1               0   \n8       4  142051.07              2          0               1   \n9       2  134603.88              1          1               1   \n\n   EstimatedSalary  Exited  \n0        101348.88       1  \n1        112542.58       0  \n2        113931.57       1  \n3         93826.63       0  \n4         79084.10       0  \n5        149756.71       1  \n6         10062.80       0  \n7        119346.88       1  \n8         74940.50       0  \n9         71725.73       0  "
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Below Info Function will print detail information on the dataset that was imported in the previous step.\ndataset.info()",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10000 entries, 0 to 9999\nData columns (total 14 columns):\nRowNumber          10000 non-null int64\nCustomerId         10000 non-null int64\nSurname            10000 non-null object\nCreditScore        10000 non-null int64\nGeography          10000 non-null object\nGender             10000 non-null object\nAge                10000 non-null int64\nTenure             10000 non-null int64\nBalance            10000 non-null float64\nNumOfProducts      10000 non-null int64\nHasCrCard          10000 non-null int64\nIsActiveMember     10000 non-null int64\nEstimatedSalary    10000 non-null float64\nExited             10000 non-null int64\ndtypes: float64(2), int64(9), object(3)\nmemory usage: 1.1+ MB\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### The above Info function tells us that there are 14 columns and 10,000 rows. This dataset contains 13 independent variables and one dependent variable (“Exited” column). Now let’s separate dataset into X(independent variables) and y(dependent variable). In X we’ll keep from third column to 13th (since we don’t need “RowNumber”,”CustomerId” and “Exited”) and in y we only need “Exited” field."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Splitting the Data Set into Dependent and Independent Variable.\nX = dataset.iloc[:, 3:13].values\ny = dataset.iloc[:, 13].values",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Below step will be performed again after encoding the categorical variables. This is being done for the final submission file."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX1_train, X1_test, y1_train, y1_test = train_test_split(X, y, test_size = 0.2, random_state = 0)",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### We have two categorical variables (“Country” and “Gender” variable) in our data. We need to encode them. I used labelencoder and onehotencoder of scikit-learn package to achieve this. Note that I removed a dummy variable of “Country” to avoid falling into dummy variable trap."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Encoding categorical data\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder_X_1 = LabelEncoder()\nX[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\nlabelencoder_X_2 = LabelEncoder()\nX[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\nonehotencoder = OneHotEncoder(categorical_features = [1])\nX = onehotencoder.fit_transform(X).toarray()\nX = X[:, 1:]\n",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Feature scaling is a method used to standardize the range of independent variables or features of data."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Now our data is well preprocessed and now we will build the artificial neural network."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Importing Tensorflow and Keras Library To Build ANN"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### To start with building the ANN we will first import Tensorflow and Kreas Library."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Part 2 - Now let's make the ANN!\nimport tensorflow as tf",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### The sequential module is required to initialize the ann and dense module is required to add layers to it. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Importing the Keras libraries and packages\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Now we will initialize the deep learning model as a sequence of layers"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Initialising the ANN\nclassifier = Sequential()\n",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### I have defined 6 units in the first hidden layer and used rectifying linear function(ReLu) as the activation function. And since input dimensions(no. of features) are 11 we will define input_dim=11."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Adding the input layer and the first hidden layer\nclassifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 11))\n",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/__main__.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=11, units=6, kernel_initializer=\"uniform\")`\n  from ipykernel import kernelapp as app\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Adding the second hidden layer\nclassifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/__main__.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n  from ipykernel import kernelapp as app\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Adding the output layer....It will have one output unit and we will use sigmoid function since it’s a binary classification."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Adding the output layer\nclassifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/__main__.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n  from ipykernel import kernelapp as app\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Compiling the ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Fitting the ANN to the Training set\nclassifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)\n",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/__main__.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n  from ipykernel import kernelapp as app\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Epoch 1/100\n8000/8000 [==============================] - 23s 3ms/step - loss: 0.4760 - acc: 0.7960\nEpoch 2/100\n8000/8000 [==============================] - 16s 2ms/step - loss: 0.4291 - acc: 0.7960\nEpoch 3/100\n8000/8000 [==============================] - 25s 3ms/step - loss: 0.4231 - acc: 0.7964\nEpoch 4/100\n8000/8000 [==============================] - 15s 2ms/step - loss: 0.4184 - acc: 0.8222\nEpoch 5/100\n8000/8000 [==============================] - 15s 2ms/step - loss: 0.4161 - acc: 0.8265\nEpoch 6/100\n8000/8000 [==============================] - 15s 2ms/step - loss: 0.4140 - acc: 0.8311\nEpoch 7/100\n8000/8000 [==============================] - 17s 2ms/step - loss: 0.4127 - acc: 0.8327\nEpoch 8/100\n8000/8000 [==============================] - 16s 2ms/step - loss: 0.4113 - acc: 0.8324\nEpoch 9/100\n8000/8000 [==============================] - 15s 2ms/step - loss: 0.4104 - acc: 0.8342\nEpoch 10/100\n8000/8000 [==============================] - 14s 2ms/step - loss: 0.4093 - acc: 0.8331\nEpoch 11/100\n8000/8000 [==============================] - 14s 2ms/step - loss: 0.4079 - acc: 0.8347\nEpoch 12/100\n8000/8000 [==============================] - 12s 1ms/step - loss: 0.4080 - acc: 0.8360\nEpoch 13/100\n8000/8000 [==============================] - 11s 1ms/step - loss: 0.4071 - acc: 0.8341\nEpoch 14/100\n8000/8000 [==============================] - 10s 1ms/step - loss: 0.4066 - acc: 0.8330: 1s \nEpoch 15/100\n8000/8000 [==============================] - 8s 1ms/step - loss: 0.4060 - acc: 0.8361A: 0s - loss: 0.4\nEpoch 16/100\n8000/8000 [==============================] - 8s 952us/step - loss: 0.4055 - acc: 0.8345\nEpoch 17/100\n8000/8000 [==============================] - 8s 992us/step - loss: 0.4055 - acc: 0.8340\nEpoch 18/100\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.4050 - acc: 0.8345\nEpoch 19/100\n8000/8000 [==============================] - 8s 943us/step - loss: 0.4041 - acc: 0.8352\nEpoch 20/100\n8000/8000 [==============================] - 8s 940us/step - loss: 0.4045 - acc: 0.8344\nEpoch 21/100\n8000/8000 [==============================] - 8s 1ms/step - loss: 0.4042 - acc: 0.8355\nEpoch 22/100\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.4037 - acc: 0.8365\nEpoch 23/100\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.4034 - acc: 0.8345\nEpoch 24/100\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.4037 - acc: 0.8340\nEpoch 25/100\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.4029 - acc: 0.8352\nEpoch 26/100\n8000/8000 [==============================] - 8s 1ms/step - loss: 0.4033 - acc: 0.8351\nEpoch 27/100\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.4030 - acc: 0.8357\nEpoch 28/100\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.4031 - acc: 0.8341\nEpoch 29/100\n8000/8000 [==============================] - 10s 1ms/step - loss: 0.4023 - acc: 0.8357\nEpoch 30/100\n8000/8000 [==============================] - 10s 1ms/step - loss: 0.4026 - acc: 0.8337\nEpoch 31/100\n8000/8000 [==============================] - 10s 1ms/step - loss: 0.4029 - acc: 0.8347\nEpoch 32/100\n8000/8000 [==============================] - 10s 1ms/step - loss: 0.4025 - acc: 0.8362\nEpoch 33/100\n8000/8000 [==============================] - 9s 1ms/step - loss: 0.4018 - acc: 0.8366\nEpoch 34/100\n8000/8000 [==============================] - 6s 798us/step - loss: 0.4024 - acc: 0.8350\nEpoch 35/100\n8000/8000 [==============================] - 6s 787us/step - loss: 0.4021 - acc: 0.8350\nEpoch 36/100\n8000/8000 [==============================] - 6s 799us/step - loss: 0.4014 - acc: 0.8355\nEpoch 37/100\n8000/8000 [==============================] - 5s 641us/step - loss: 0.4013 - acc: 0.8344\nEpoch 38/100\n8000/8000 [==============================] - 6s 780us/step - loss: 0.4016 - acc: 0.8366 0s - loss: 0.4018 - acc: 0.8\nEpoch 39/100\n8000/8000 [==============================] - 7s 817us/step - loss: 0.4012 - acc: 0.8359\nEpoch 40/100\n8000/8000 [==============================] - 6s 785us/step - loss: 0.4015 - acc: 0.8360\nEpoch 41/100\n8000/8000 [==============================] - 6s 756us/step - loss: 0.4009 - acc: 0.8365\nEpoch 42/100\n8000/8000 [==============================] - 7s 820us/step - loss: 0.4010 - acc: 0.8349\nEpoch 43/100\n8000/8000 [==============================] - 6s 807us/step - loss: 0.4011 - acc: 0.8359\nEpoch 44/100\n8000/8000 [==============================] - 6s 741us/step - loss: 0.4009 - acc: 0.8357\nEpoch 45/100\n8000/8000 [==============================] - 6s 787us/step - loss: 0.4009 - acc: 0.8357\nEpoch 46/100\n8000/8000 [==============================] - 7s 825us/step - loss: 0.4009 - acc: 0.8351\nEpoch 47/100\n8000/8000 [==============================] - 6s 748us/step - loss: 0.4012 - acc: 0.8361\nEpoch 48/100\n8000/8000 [==============================] - 6s 695us/step - loss: 0.4009 - acc: 0.8345\nEpoch 49/100\n8000/8000 [==============================] - 6s 730us/step - loss: 0.4005 - acc: 0.8339\nEpoch 50/100\n8000/8000 [==============================] - 5s 672us/step - loss: 0.4008 - acc: 0.8360\nEpoch 51/100\n8000/8000 [==============================] - 6s 711us/step - loss: 0.4008 - acc: 0.8359\nEpoch 52/100\n8000/8000 [==============================] - 5s 654us/step - loss: 0.4005 - acc: 0.8366\nEpoch 53/100\n8000/8000 [==============================] - 5s 638us/step - loss: 0.4010 - acc: 0.8347\nEpoch 54/100\n8000/8000 [==============================] - 5s 645us/step - loss: 0.4003 - acc: 0.8366\nEpoch 55/100\n8000/8000 [==============================] - 5s 657us/step - loss: 0.4008 - acc: 0.8361\nEpoch 56/100\n8000/8000 [==============================] - 6s 692us/step - loss: 0.4004 - acc: 0.8360\nEpoch 57/100\n8000/8000 [==============================] - 5s 673us/step - loss: 0.4002 - acc: 0.8352\nEpoch 58/100\n8000/8000 [==============================] - 5s 591us/step - loss: 0.3997 - acc: 0.8364\nEpoch 59/100\n8000/8000 [==============================] - 5s 645us/step - loss: 0.4000 - acc: 0.8349\nEpoch 60/100\n8000/8000 [==============================] - 5s 626us/step - loss: 0.4000 - acc: 0.8346\nEpoch 61/100\n8000/8000 [==============================] - 6s 692us/step - loss: 0.3997 - acc: 0.8344\nEpoch 62/100\n8000/8000 [==============================] - 5s 630us/step - loss: 0.3998 - acc: 0.8375\nEpoch 63/100\n8000/8000 [==============================] - 6s 704us/step - loss: 0.3996 - acc: 0.8352\nEpoch 64/100\n8000/8000 [==============================] - 5s 614us/step - loss: 0.4004 - acc: 0.8351\nEpoch 65/100\n8000/8000 [==============================] - 5s 645us/step - loss: 0.3994 - acc: 0.8360\nEpoch 66/100\n8000/8000 [==============================] - 5s 653us/step - loss: 0.3994 - acc: 0.8361\nEpoch 67/100\n8000/8000 [==============================] - 6s 705us/step - loss: 0.3995 - acc: 0.8362\nEpoch 68/100\n8000/8000 [==============================] - 6s 709us/step - loss: 0.3995 - acc: 0.8355\nEpoch 69/100\n8000/8000 [==============================] - 5s 605us/step - loss: 0.3994 - acc: 0.8362\nEpoch 70/100\n8000/8000 [==============================] - 5s 624us/step - loss: 0.3995 - acc: 0.8364\nEpoch 71/100\n8000/8000 [==============================] - 5s 611us/step - loss: 0.3995 - acc: 0.8359\nEpoch 72/100\n8000/8000 [==============================] - 5s 639us/step - loss: 0.3989 - acc: 0.8357\nEpoch 73/100\n8000/8000 [==============================] - 5s 666us/step - loss: 0.3994 - acc: 0.8362\nEpoch 74/100\n8000/8000 [==============================] - 6s 696us/step - loss: 0.3992 - acc: 0.8355\nEpoch 75/100\n8000/8000 [==============================] - 5s 672us/step - loss: 0.3991 - acc: 0.8361\nEpoch 76/100\n8000/8000 [==============================] - 5s 637us/step - loss: 0.3991 - acc: 0.8376\nEpoch 77/100\n8000/8000 [==============================] - 5s 601us/step - loss: 0.3984 - acc: 0.8356\nEpoch 78/100\n8000/8000 [==============================] - 5s 665us/step - loss: 0.3992 - acc: 0.8360\nEpoch 79/100\n8000/8000 [==============================] - 6s 738us/step - loss: 0.3990 - acc: 0.8350\nEpoch 80/100\n8000/8000 [==============================] - 5s 650us/step - loss: 0.3990 - acc: 0.8364\nEpoch 81/100\n8000/8000 [==============================] - 5s 593us/step - loss: 0.3991 - acc: 0.8360\nEpoch 82/100\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "8000/8000 [==============================] - 5s 647us/step - loss: 0.3986 - acc: 0.8367\nEpoch 83/100\n8000/8000 [==============================] - 5s 646us/step - loss: 0.3986 - acc: 0.8362\nEpoch 84/100\n8000/8000 [==============================] - 5s 644us/step - loss: 0.3988 - acc: 0.8360\nEpoch 85/100\n8000/8000 [==============================] - 5s 636us/step - loss: 0.3991 - acc: 0.8362\nEpoch 86/100\n8000/8000 [==============================] - 5s 674us/step - loss: 0.3988 - acc: 0.8355\nEpoch 87/100\n8000/8000 [==============================] - 5s 658us/step - loss: 0.3989 - acc: 0.8355\nEpoch 88/100\n8000/8000 [==============================] - 5s 652us/step - loss: 0.3987 - acc: 0.8351\nEpoch 89/100\n8000/8000 [==============================] - 5s 614us/step - loss: 0.3986 - acc: 0.8369\nEpoch 90/100\n8000/8000 [==============================] - 6s 726us/step - loss: 0.3987 - acc: 0.8361\nEpoch 91/100\n8000/8000 [==============================] - 5s 681us/step - loss: 0.3980 - acc: 0.8354\nEpoch 92/100\n8000/8000 [==============================] - 5s 643us/step - loss: 0.3982 - acc: 0.8360\nEpoch 93/100\n8000/8000 [==============================] - 5s 627us/step - loss: 0.3984 - acc: 0.8361\nEpoch 94/100\n8000/8000 [==============================] - 5s 657us/step - loss: 0.3980 - acc: 0.8366\nEpoch 95/100\n8000/8000 [==============================] - 5s 665us/step - loss: 0.3985 - acc: 0.8361\nEpoch 96/100\n8000/8000 [==============================] - 5s 649us/step - loss: 0.3988 - acc: 0.8362\nEpoch 97/100\n8000/8000 [==============================] - 5s 683us/step - loss: 0.3986 - acc: 0.8364\nEpoch 98/100\n8000/8000 [==============================] - 5s 647us/step - loss: 0.3982 - acc: 0.8377\nEpoch 99/100\n8000/8000 [==============================] - 5s 611us/step - loss: 0.3983 - acc: 0.8361\nEpoch 100/100\n8000/8000 [==============================] - 5s 633us/step - loss: 0.3984 - acc: 0.8351\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "<keras.callbacks.History at 0x7f1d141ac630>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Part 3 - Making the predictions and evaluating the model\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\ny_pred = (y_pred > 0.5)\n",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)",
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "cm",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": "array([[1541,   54],\n       [ 255,  150]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Merging Test Data and Predicted Test Results. Writing the combined data set into CSV File."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df1 = pd.DataFrame(X1_test)\ndf2 = pd.DataFrame(y_pred)\nsubmission = pd.concat([df1,df2],axis = 1)\nsubmission.to_csv('submission.csv', index=False)\nsubmission",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 24,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>597</td>\n      <td>Germany</td>\n      <td>Female</td>\n      <td>35</td>\n      <td>8</td>\n      <td>131101</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>192853</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>523</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>40</td>\n      <td>2</td>\n      <td>102967</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>128702</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>706</td>\n      <td>Spain</td>\n      <td>Female</td>\n      <td>42</td>\n      <td>8</td>\n      <td>95386.8</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>75732.2</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>788</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>32</td>\n      <td>4</td>\n      <td>112080</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>89368.6</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>706</td>\n      <td>Germany</td>\n      <td>Male</td>\n      <td>38</td>\n      <td>5</td>\n      <td>163035</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>135662</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>670</td>\n      <td>Spain</td>\n      <td>Female</td>\n      <td>57</td>\n      <td>3</td>\n      <td>175576</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>99061.8</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>590</td>\n      <td>Spain</td>\n      <td>Male</td>\n      <td>34</td>\n      <td>0</td>\n      <td>65812.4</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>160346</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>636</td>\n      <td>Spain</td>\n      <td>Female</td>\n      <td>29</td>\n      <td>6</td>\n      <td>157576</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>101102</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>598</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>64</td>\n      <td>9</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>13181.4</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>456</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>63</td>\n      <td>1</td>\n      <td>165351</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>140758</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>498</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>31</td>\n      <td>10</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>13892.6</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>714</td>\n      <td>Spain</td>\n      <td>Male</td>\n      <td>45</td>\n      <td>8</td>\n      <td>150900</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>139889</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>488</td>\n      <td>Germany</td>\n      <td>Female</td>\n      <td>33</td>\n      <td>4</td>\n      <td>140002</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>123614</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>562</td>\n      <td>Germany</td>\n      <td>Female</td>\n      <td>31</td>\n      <td>9</td>\n      <td>117153</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>108675</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>772</td>\n      <td>Germany</td>\n      <td>Female</td>\n      <td>51</td>\n      <td>9</td>\n      <td>143931</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>46675.5</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>667</td>\n      <td>Germany</td>\n      <td>Male</td>\n      <td>55</td>\n      <td>9</td>\n      <td>154393</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>137675</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>513</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>43</td>\n      <td>9</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>152500</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>675</td>\n      <td>Spain</td>\n      <td>Male</td>\n      <td>66</td>\n      <td>5</td>\n      <td>115654</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>131971</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>622</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>30</td>\n      <td>4</td>\n      <td>107879</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>196895</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>551</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>45</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>51143.4</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>663</td>\n      <td>Germany</td>\n      <td>Male</td>\n      <td>42</td>\n      <td>7</td>\n      <td>115931</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>19862.8</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>850</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>35</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>56991.7</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>552</td>\n      <td>Spain</td>\n      <td>Female</td>\n      <td>34</td>\n      <td>4</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>140287</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>444</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>34</td>\n      <td>2</td>\n      <td>144319</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>112668</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>794</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>35</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>68730.9</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>827</td>\n      <td>Spain</td>\n      <td>Male</td>\n      <td>46</td>\n      <td>1</td>\n      <td>183276</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>13460.3</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>590</td>\n      <td>Spain</td>\n      <td>Female</td>\n      <td>29</td>\n      <td>2</td>\n      <td>166931</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>122488</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>492</td>\n      <td>Germany</td>\n      <td>Female</td>\n      <td>30</td>\n      <td>10</td>\n      <td>77168.9</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>146700</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>498</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>29</td>\n      <td>9</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>190036</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>757</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>44</td>\n      <td>4</td>\n      <td>123322</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>137136</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1970</th>\n      <td>640</td>\n      <td>Spain</td>\n      <td>Male</td>\n      <td>47</td>\n      <td>6</td>\n      <td>89047.1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>116286</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1971</th>\n      <td>624</td>\n      <td>Spain</td>\n      <td>Male</td>\n      <td>46</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>62825</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1972</th>\n      <td>660</td>\n      <td>Germany</td>\n      <td>Male</td>\n      <td>28</td>\n      <td>1</td>\n      <td>118402</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>14288.9</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1973</th>\n      <td>721</td>\n      <td>Spain</td>\n      <td>Male</td>\n      <td>38</td>\n      <td>7</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>53534.8</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1974</th>\n      <td>803</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>33</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>115677</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1975</th>\n      <td>543</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>71</td>\n      <td>1</td>\n      <td>104309</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>25650</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1976</th>\n      <td>712</td>\n      <td>Spain</td>\n      <td>Male</td>\n      <td>47</td>\n      <td>1</td>\n      <td>139887</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>95719.7</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1977</th>\n      <td>714</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>34</td>\n      <td>5</td>\n      <td>141173</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>98896.1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1978</th>\n      <td>696</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>27</td>\n      <td>4</td>\n      <td>87637.3</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>196111</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1979</th>\n      <td>715</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>28</td>\n      <td>7</td>\n      <td>160377</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>196853</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1980</th>\n      <td>528</td>\n      <td>Spain</td>\n      <td>Male</td>\n      <td>43</td>\n      <td>7</td>\n      <td>97473.9</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>159823</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1981</th>\n      <td>535</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>49</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>61820.4</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1982</th>\n      <td>522</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>41</td>\n      <td>5</td>\n      <td>144148</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>14789.9</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1983</th>\n      <td>528</td>\n      <td>Spain</td>\n      <td>Male</td>\n      <td>23</td>\n      <td>7</td>\n      <td>104745</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>170263</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1984</th>\n      <td>575</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>29</td>\n      <td>4</td>\n      <td>121823</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>50368.9</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1985</th>\n      <td>629</td>\n      <td>Spain</td>\n      <td>Male</td>\n      <td>41</td>\n      <td>10</td>\n      <td>150149</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6936.27</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1986</th>\n      <td>525</td>\n      <td>Spain</td>\n      <td>Female</td>\n      <td>25</td>\n      <td>10</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>69361.9</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1987</th>\n      <td>511</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>27</td>\n      <td>8</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>49089.4</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1988</th>\n      <td>699</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>44</td>\n      <td>8</td>\n      <td>158698</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>107181</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1989</th>\n      <td>583</td>\n      <td>Spain</td>\n      <td>Female</td>\n      <td>40</td>\n      <td>3</td>\n      <td>54428.4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>109639</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1990</th>\n      <td>494</td>\n      <td>Spain</td>\n      <td>Male</td>\n      <td>67</td>\n      <td>5</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>85890.2</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1991</th>\n      <td>743</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>30</td>\n      <td>1</td>\n      <td>127023</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>138781</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1992</th>\n      <td>593</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>36</td>\n      <td>2</td>\n      <td>70181.5</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>80608.1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1993</th>\n      <td>611</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>28</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>25395.8</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1994</th>\n      <td>439</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>28</td>\n      <td>7</td>\n      <td>110976</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>138527</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1995</th>\n      <td>625</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>24</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>180970</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1996</th>\n      <td>586</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>35</td>\n      <td>7</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>70760.7</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1997</th>\n      <td>578</td>\n      <td>Spain</td>\n      <td>Male</td>\n      <td>36</td>\n      <td>1</td>\n      <td>157268</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>141533</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1998</th>\n      <td>650</td>\n      <td>Germany</td>\n      <td>Male</td>\n      <td>34</td>\n      <td>4</td>\n      <td>142393</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>11276.5</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1999</th>\n      <td>573</td>\n      <td>Germany</td>\n      <td>Male</td>\n      <td>30</td>\n      <td>8</td>\n      <td>127406</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>192951</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>2000 rows × 11 columns</p>\n</div>",
            "text/plain": "        0        1       2   3   4        5  6  7  8        9      0\n0     597  Germany  Female  35   8   131101  1  1  1   192853  False\n1     523   France  Female  40   2   102967  1  1  0   128702  False\n2     706    Spain  Female  42   8  95386.8  1  1  1  75732.2  False\n3     788   France    Male  32   4   112080  1  0  0  89368.6  False\n4     706  Germany    Male  38   5   163035  2  1  1   135662  False\n5     670    Spain  Female  57   3   175576  2  1  0  99061.8   True\n6     590    Spain    Male  34   0  65812.4  2  0  1   160346  False\n7     636    Spain  Female  29   6   157576  2  1  1   101102  False\n8     598   France  Female  64   9        0  1  0  1  13181.4  False\n9     456   France  Female  63   1   165351  2  0  0   140758   True\n10    498   France  Female  31  10        0  2  1  0  13892.6  False\n11    714    Spain    Male  45   8   150900  2  0  1   139889  False\n12    488  Germany  Female  33   4   140002  1  1  0   123614  False\n13    562  Germany  Female  31   9   117153  1  1  1   108675  False\n14    772  Germany  Female  51   9   143931  1  0  1  46675.5   True\n15    667  Germany    Male  55   9   154393  1  1  1   137675  False\n16    513   France  Female  43   9        0  2  1  0   152500  False\n17    675    Spain    Male  66   5   115654  2  1  1   131971  False\n18    622   France  Female  30   4   107879  1  0  1   196895  False\n19    551   France  Female  45   6        0  2  1  1  51143.4  False\n20    663  Germany    Male  42   7   115931  1  1  0  19862.8  False\n21    850   France    Male  35   2        0  2  1  1  56991.7  False\n22    552    Spain  Female  34   4        0  2  1  0   140287  False\n23    444   France    Male  34   2   144319  1  1  0   112668  False\n24    794   France    Male  35   6        0  2  1  1  68730.9  False\n25    827    Spain    Male  46   1   183276  1  1  1  13460.3  False\n26    590    Spain  Female  29   2   166931  2  1  0   122488  False\n27    492  Germany  Female  30  10  77168.9  2  0  1   146700  False\n28    498   France  Female  29   9        0  1  1  0   190036  False\n29    757   France  Female  44   4   123322  1  1  0   137136  False\n...   ...      ...     ...  ..  ..      ... .. .. ..      ...    ...\n1970  640    Spain    Male  47   6  89047.1  1  1  0   116286  False\n1971  624    Spain    Male  46   3        0  2  1  1    62825  False\n1972  660  Germany    Male  28   1   118402  2  1  0  14288.9  False\n1973  721    Spain    Male  38   7        0  1  0  1  53534.8  False\n1974  803   France    Male  33   6        0  2  1  0   115677  False\n1975  543   France  Female  71   1   104309  1  1  1    25650  False\n1976  712    Spain    Male  47   1   139887  1  1  1  95719.7  False\n1977  714   France    Male  34   5   141173  1  0  1  98896.1  False\n1978  696   France    Male  27   4  87637.3  2  0  0   196111  False\n1979  715   France    Male  28   7   160377  1  0  0   196853  False\n1980  528    Spain    Male  43   7  97473.9  2  1  1   159823  False\n1981  535   France  Female  49   3        0  1  0  0  61820.4   True\n1982  522   France    Male  41   5   144148  1  1  1  14789.9  False\n1983  528    Spain    Male  23   7   104745  1  1  0   170263  False\n1984  575   France    Male  29   4   121823  2  1  1  50368.9  False\n1985  629    Spain    Male  41  10   150149  1  0  0  6936.27  False\n1986  525    Spain  Female  25  10        0  2  1  0  69361.9  False\n1987  511   France    Male  27   8        0  2  1  1  49089.4  False\n1988  699   France    Male  44   8   158698  1  1  0   107181  False\n1989  583    Spain  Female  40   3  54428.4  1  1  0   109639  False\n1990  494    Spain    Male  67   5        0  2  1  1  85890.2  False\n1991  743   France  Female  30   1   127023  1  1  1   138781  False\n1992  593   France    Male  36   2  70181.5  2  1  0  80608.1  False\n1993  611   France    Male  28   2        0  2  0  0  25395.8  False\n1994  439   France    Male  28   7   110976  2  1  0   138527  False\n1995  625   France  Female  24   1        0  2  1  1   180970  False\n1996  586   France  Female  35   7        0  2  1  0  70760.7  False\n1997  578    Spain    Male  36   1   157268  2  1  0   141533  False\n1998  650  Germany    Male  34   4   142393  1  1  1  11276.5  False\n1999  573  Germany    Male  30   8   127406  1  1  0   192951  False\n\n[2000 rows x 11 columns]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}