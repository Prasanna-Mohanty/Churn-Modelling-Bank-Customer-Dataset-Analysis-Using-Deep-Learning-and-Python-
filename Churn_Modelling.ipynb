{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Churn Modelling Data Set Analysis Using ANN"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Data Preprocessing"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Part 1 - Data Preprocessing\n\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Importing the dataset\ndataset = pd.read_csv('Churn_Modelling.csv')\ndataset.head(10)",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RowNumber</th>\n      <th>CustomerId</th>\n      <th>Surname</th>\n      <th>CreditScore</th>\n      <th>Geography</th>\n      <th>Gender</th>\n      <th>Age</th>\n      <th>Tenure</th>\n      <th>Balance</th>\n      <th>NumOfProducts</th>\n      <th>HasCrCard</th>\n      <th>IsActiveMember</th>\n      <th>EstimatedSalary</th>\n      <th>Exited</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>15634602</td>\n      <td>Hargrave</td>\n      <td>619</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>42</td>\n      <td>2</td>\n      <td>0.00</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>101348.88</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>15647311</td>\n      <td>Hill</td>\n      <td>608</td>\n      <td>Spain</td>\n      <td>Female</td>\n      <td>41</td>\n      <td>1</td>\n      <td>83807.86</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>112542.58</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>15619304</td>\n      <td>Onio</td>\n      <td>502</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>42</td>\n      <td>8</td>\n      <td>159660.80</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113931.57</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>15701354</td>\n      <td>Boni</td>\n      <td>699</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>39</td>\n      <td>1</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>93826.63</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>15737888</td>\n      <td>Mitchell</td>\n      <td>850</td>\n      <td>Spain</td>\n      <td>Female</td>\n      <td>43</td>\n      <td>2</td>\n      <td>125510.82</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>79084.10</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>15574012</td>\n      <td>Chu</td>\n      <td>645</td>\n      <td>Spain</td>\n      <td>Male</td>\n      <td>44</td>\n      <td>8</td>\n      <td>113755.78</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>149756.71</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>15592531</td>\n      <td>Bartlett</td>\n      <td>822</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>50</td>\n      <td>7</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>10062.80</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>15656148</td>\n      <td>Obinna</td>\n      <td>376</td>\n      <td>Germany</td>\n      <td>Female</td>\n      <td>29</td>\n      <td>4</td>\n      <td>115046.74</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>119346.88</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>15792365</td>\n      <td>He</td>\n      <td>501</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>44</td>\n      <td>4</td>\n      <td>142051.07</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>74940.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>15592389</td>\n      <td>H?</td>\n      <td>684</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>27</td>\n      <td>2</td>\n      <td>134603.88</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>71725.73</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n0          1    15634602  Hargrave          619    France  Female   42   \n1          2    15647311      Hill          608     Spain  Female   41   \n2          3    15619304      Onio          502    France  Female   42   \n3          4    15701354      Boni          699    France  Female   39   \n4          5    15737888  Mitchell          850     Spain  Female   43   \n5          6    15574012       Chu          645     Spain    Male   44   \n6          7    15592531  Bartlett          822    France    Male   50   \n7          8    15656148    Obinna          376   Germany  Female   29   \n8          9    15792365        He          501    France    Male   44   \n9         10    15592389        H?          684    France    Male   27   \n\n   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n0       2       0.00              1          1               1   \n1       1   83807.86              1          0               1   \n2       8  159660.80              3          1               0   \n3       1       0.00              2          0               0   \n4       2  125510.82              1          1               1   \n5       8  113755.78              2          1               0   \n6       7       0.00              2          1               1   \n7       4  115046.74              4          1               0   \n8       4  142051.07              2          0               1   \n9       2  134603.88              1          1               1   \n\n   EstimatedSalary  Exited  \n0        101348.88       1  \n1        112542.58       0  \n2        113931.57       1  \n3         93826.63       0  \n4         79084.10       0  \n5        149756.71       1  \n6         10062.80       0  \n7        119346.88       1  \n8         74940.50       0  \n9         71725.73       0  "
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Below Info Function will print detail information on the dataset that was imported in the previous step.\ndataset.info()",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10000 entries, 0 to 9999\nData columns (total 14 columns):\nRowNumber          10000 non-null int64\nCustomerId         10000 non-null int64\nSurname            10000 non-null object\nCreditScore        10000 non-null int64\nGeography          10000 non-null object\nGender             10000 non-null object\nAge                10000 non-null int64\nTenure             10000 non-null int64\nBalance            10000 non-null float64\nNumOfProducts      10000 non-null int64\nHasCrCard          10000 non-null int64\nIsActiveMember     10000 non-null int64\nEstimatedSalary    10000 non-null float64\nExited             10000 non-null int64\ndtypes: float64(2), int64(9), object(3)\nmemory usage: 1.1+ MB\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### The above Info function tells us that there are 14 columns and 10,000 rows. This dataset contains 13 independent variables and one dependent variable (“Exited” column). Now let’s separate dataset into X(independent variables) and y(dependent variable). In X we’ll keep from third column to 13th (since we don’t need “RowNumber”,”CustomerId” and “Exited”) and in y we only need “Exited” field."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Splitting the Data Set into Dependent and Independent Variable.\nX = dataset.iloc[:, 3:13].values\ny = dataset.iloc[:, 13].values",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### We have two categorical variables (“Country” and “Gender” variable) in our data. We need to encode them. I used labelencoder and onehotencoder of scikit-learn package to achieve this. Note that I removed a dummy variable of “Country” to avoid falling into dummy variable trap."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Encoding categorical data\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder_X_1 = LabelEncoder()\nX[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\nlabelencoder_X_2 = LabelEncoder()\nX[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\nonehotencoder = OneHotEncoder(categorical_features = [1])\nX = onehotencoder.fit_transform(X).toarray()\nX = X[:, 1:]\n",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Feature scaling is a method used to standardize the range of independent variables or features of data."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Now our data is well preprocessed and now we will build the artificial neural network."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Importing Tensorflow and Keras Library To Build ANN"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### To start with building the ANN we will first import Tensorflow and Kreas Library."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Part 2 - Now let's make the ANN!\nimport tensorflow as tf",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### The sequential module is required to initialize the ann and dense module is required to add layers to it. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Importing the Keras libraries and packages\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Now we will initialize the deep learning model as a sequence of layers"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Initialising the ANN\nclassifier = Sequential()\n",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### I have defined 6 units in the first hidden layer and used rectifying linear function(ReLu) as the activation function. And since input dimensions(no. of features) are 11 we will define input_dim=11."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Adding the input layer and the first hidden layer\nclassifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 11))\n",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/__main__.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=11, units=6, kernel_initializer=\"uniform\")`\n  from ipykernel import kernelapp as app\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Adding the second hidden layer\nclassifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/__main__.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n  from ipykernel import kernelapp as app\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Adding the output layer....It will have one output unit and we will use sigmoid function since it’s a binary classification."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Adding the output layer\nclassifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/__main__.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n  from ipykernel import kernelapp as app\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Compiling the ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Fitting the ANN to the Training set\nclassifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)\n",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/__main__.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n  from ipykernel import kernelapp as app\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Epoch 1/100\n8000/8000 [==============================] - 1s 172us/step - loss: 0.4873 - acc: 0.7947 0s - loss: 0.5370 - \nEpoch 2/100\n8000/8000 [==============================] - 1s 139us/step - loss: 0.4298 - acc: 0.7960\nEpoch 3/100\n8000/8000 [==============================] - 1s 151us/step - loss: 0.4255 - acc: 0.7960\nEpoch 4/100\n8000/8000 [==============================] - 1s 136us/step - loss: 0.4215 - acc: 0.8074\nEpoch 5/100\n8000/8000 [==============================] - 1s 145us/step - loss: 0.4188 - acc: 0.8210\nEpoch 6/100\n8000/8000 [==============================] - 1s 134us/step - loss: 0.4164 - acc: 0.8275\nEpoch 7/100\n8000/8000 [==============================] - 1s 138us/step - loss: 0.4145 - acc: 0.8294\nEpoch 8/100\n8000/8000 [==============================] - 1s 132us/step - loss: 0.4127 - acc: 0.8310\nEpoch 9/100\n8000/8000 [==============================] - 1s 132us/step - loss: 0.4114 - acc: 0.8330\nEpoch 10/100\n8000/8000 [==============================] - 1s 135us/step - loss: 0.4100 - acc: 0.8325\nEpoch 11/100\n8000/8000 [==============================] - 1s 132us/step - loss: 0.4091 - acc: 0.8347\nEpoch 12/100\n8000/8000 [==============================] - 1s 135us/step - loss: 0.4082 - acc: 0.8344\nEpoch 13/100\n8000/8000 [==============================] - 1s 132us/step - loss: 0.4076 - acc: 0.8335\nEpoch 14/100\n8000/8000 [==============================] - 1s 139us/step - loss: 0.4068 - acc: 0.8344\nEpoch 15/100\n8000/8000 [==============================] - 1s 148us/step - loss: 0.4063 - acc: 0.8342\nEpoch 16/100\n8000/8000 [==============================] - 1s 136us/step - loss: 0.4054 - acc: 0.8345\nEpoch 17/100\n8000/8000 [==============================] - 1s 130us/step - loss: 0.4056 - acc: 0.8345\nEpoch 18/100\n8000/8000 [==============================] - 1s 133us/step - loss: 0.4050 - acc: 0.8335\nEpoch 19/100\n8000/8000 [==============================] - 1s 133us/step - loss: 0.4045 - acc: 0.8352\nEpoch 20/100\n8000/8000 [==============================] - 1s 133us/step - loss: 0.4046 - acc: 0.8356\nEpoch 21/100\n8000/8000 [==============================] - 1s 132us/step - loss: 0.4039 - acc: 0.8336\nEpoch 22/100\n8000/8000 [==============================] - 1s 135us/step - loss: 0.4038 - acc: 0.8354\nEpoch 23/100\n8000/8000 [==============================] - 1s 138us/step - loss: 0.4029 - acc: 0.8354\nEpoch 24/100\n8000/8000 [==============================] - 1s 135us/step - loss: 0.4032 - acc: 0.8344\nEpoch 25/100\n8000/8000 [==============================] - 1s 138us/step - loss: 0.4032 - acc: 0.8337\nEpoch 26/100\n8000/8000 [==============================] - 1s 135us/step - loss: 0.4028 - acc: 0.8339\nEpoch 27/100\n8000/8000 [==============================] - 1s 151us/step - loss: 0.4032 - acc: 0.8346\nEpoch 28/100\n8000/8000 [==============================] - 1s 145us/step - loss: 0.4025 - acc: 0.8355\nEpoch 29/100\n8000/8000 [==============================] - 1s 140us/step - loss: 0.4025 - acc: 0.8337\nEpoch 30/100\n8000/8000 [==============================] - 1s 145us/step - loss: 0.4022 - acc: 0.8357\nEpoch 31/100\n8000/8000 [==============================] - 1s 146us/step - loss: 0.4024 - acc: 0.8342\nEpoch 32/100\n8000/8000 [==============================] - 1s 137us/step - loss: 0.4025 - acc: 0.8337\nEpoch 33/100\n8000/8000 [==============================] - 1s 134us/step - loss: 0.4016 - acc: 0.8354\nEpoch 34/100\n8000/8000 [==============================] - 1s 135us/step - loss: 0.4018 - acc: 0.8355\nEpoch 35/100\n8000/8000 [==============================] - 1s 134us/step - loss: 0.4017 - acc: 0.8344\nEpoch 36/100\n8000/8000 [==============================] - 1s 137us/step - loss: 0.4017 - acc: 0.8335\nEpoch 37/100\n8000/8000 [==============================] - 1s 146us/step - loss: 0.4020 - acc: 0.8354\nEpoch 38/100\n8000/8000 [==============================] - 1s 138us/step - loss: 0.4017 - acc: 0.8354\nEpoch 39/100\n8000/8000 [==============================] - 1s 137us/step - loss: 0.4013 - acc: 0.8346\nEpoch 40/100\n8000/8000 [==============================] - 1s 132us/step - loss: 0.4010 - acc: 0.8366\nEpoch 41/100\n8000/8000 [==============================] - 1s 133us/step - loss: 0.4012 - acc: 0.8352\nEpoch 42/100\n8000/8000 [==============================] - 1s 132us/step - loss: 0.4011 - acc: 0.8347\nEpoch 43/100\n8000/8000 [==============================] - 1s 138us/step - loss: 0.4015 - acc: 0.8349\nEpoch 44/100\n8000/8000 [==============================] - 1s 147us/step - loss: 0.4013 - acc: 0.8354\nEpoch 45/100\n8000/8000 [==============================] - 1s 138us/step - loss: 0.4015 - acc: 0.8354\nEpoch 46/100\n8000/8000 [==============================] - 1s 136us/step - loss: 0.4013 - acc: 0.8362\nEpoch 47/100\n8000/8000 [==============================] - 1s 149us/step - loss: 0.4011 - acc: 0.8332\nEpoch 48/100\n8000/8000 [==============================] - 1s 135us/step - loss: 0.4007 - acc: 0.8345\nEpoch 49/100\n8000/8000 [==============================] - 1s 141us/step - loss: 0.4010 - acc: 0.8360\nEpoch 50/100\n8000/8000 [==============================] - 1s 144us/step - loss: 0.4005 - acc: 0.8344\nEpoch 51/100\n8000/8000 [==============================] - 1s 151us/step - loss: 0.4012 - acc: 0.8332\nEpoch 52/100\n8000/8000 [==============================] - 1s 139us/step - loss: 0.4008 - acc: 0.8347\nEpoch 53/100\n8000/8000 [==============================] - 1s 137us/step - loss: 0.4009 - acc: 0.8347\nEpoch 54/100\n8000/8000 [==============================] - 1s 135us/step - loss: 0.4009 - acc: 0.8354\nEpoch 55/100\n8000/8000 [==============================] - 1s 141us/step - loss: 0.4010 - acc: 0.8335\nEpoch 56/100\n8000/8000 [==============================] - 1s 137us/step - loss: 0.4009 - acc: 0.8345\nEpoch 57/100\n8000/8000 [==============================] - 1s 143us/step - loss: 0.4007 - acc: 0.8354\nEpoch 58/100\n8000/8000 [==============================] - 1s 146us/step - loss: 0.4009 - acc: 0.8352\nEpoch 59/100\n8000/8000 [==============================] - 1s 136us/step - loss: 0.4005 - acc: 0.8340\nEpoch 60/100\n8000/8000 [==============================] - 1s 135us/step - loss: 0.4004 - acc: 0.8347\nEpoch 61/100\n8000/8000 [==============================] - 1s 143us/step - loss: 0.4006 - acc: 0.8367\nEpoch 62/100\n8000/8000 [==============================] - 1s 140us/step - loss: 0.4007 - acc: 0.8354\nEpoch 63/100\n8000/8000 [==============================] - 1s 139us/step - loss: 0.4004 - acc: 0.8367\nEpoch 64/100\n8000/8000 [==============================] - 1s 135us/step - loss: 0.4009 - acc: 0.8355\nEpoch 65/100\n8000/8000 [==============================] - 1s 131us/step - loss: 0.4007 - acc: 0.8369\nEpoch 66/100\n8000/8000 [==============================] - 1s 149us/step - loss: 0.4001 - acc: 0.8361\nEpoch 67/100\n8000/8000 [==============================] - 1s 137us/step - loss: 0.4008 - acc: 0.8349\nEpoch 68/100\n8000/8000 [==============================] - 1s 133us/step - loss: 0.4007 - acc: 0.8342\nEpoch 69/100\n8000/8000 [==============================] - 1s 133us/step - loss: 0.4006 - acc: 0.8359\nEpoch 70/100\n8000/8000 [==============================] - 1s 135us/step - loss: 0.4008 - acc: 0.8345\nEpoch 71/100\n8000/8000 [==============================] - 1s 133us/step - loss: 0.4002 - acc: 0.8361\nEpoch 72/100\n8000/8000 [==============================] - 1s 136us/step - loss: 0.4006 - acc: 0.8367\nEpoch 73/100\n8000/8000 [==============================] - 1s 144us/step - loss: 0.4008 - acc: 0.8334\nEpoch 74/100\n8000/8000 [==============================] - 1s 140us/step - loss: 0.4006 - acc: 0.8350\nEpoch 75/100\n8000/8000 [==============================] - 1s 145us/step - loss: 0.4003 - acc: 0.8361\nEpoch 76/100\n8000/8000 [==============================] - 1s 139us/step - loss: 0.4004 - acc: 0.8352\nEpoch 77/100\n8000/8000 [==============================] - 1s 131us/step - loss: 0.4007 - acc: 0.8366\nEpoch 78/100\n8000/8000 [==============================] - 1s 134us/step - loss: 0.4006 - acc: 0.8335\nEpoch 79/100\n8000/8000 [==============================] - 1s 133us/step - loss: 0.4003 - acc: 0.8376\nEpoch 80/100\n8000/8000 [==============================] - 1s 137us/step - loss: 0.4002 - acc: 0.8362\nEpoch 81/100\n8000/8000 [==============================] - 1s 165us/step - loss: 0.4003 - acc: 0.8356\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "Epoch 82/100\n8000/8000 [==============================] - 1s 146us/step - loss: 0.4005 - acc: 0.8355\nEpoch 83/100\n8000/8000 [==============================] - 1s 141us/step - loss: 0.4002 - acc: 0.8361\nEpoch 84/100\n8000/8000 [==============================] - 1s 149us/step - loss: 0.4002 - acc: 0.8360\nEpoch 85/100\n8000/8000 [==============================] - 1s 140us/step - loss: 0.4005 - acc: 0.8359\nEpoch 86/100\n8000/8000 [==============================] - 1s 133us/step - loss: 0.4000 - acc: 0.8355\nEpoch 87/100\n8000/8000 [==============================] - 1s 133us/step - loss: 0.4004 - acc: 0.8364\nEpoch 88/100\n8000/8000 [==============================] - 1s 131us/step - loss: 0.4003 - acc: 0.8369\nEpoch 89/100\n8000/8000 [==============================] - 1s 131us/step - loss: 0.4004 - acc: 0.8352\nEpoch 90/100\n8000/8000 [==============================] - 1s 142us/step - loss: 0.4003 - acc: 0.8357\nEpoch 91/100\n8000/8000 [==============================] - 1s 137us/step - loss: 0.4002 - acc: 0.8350\nEpoch 92/100\n8000/8000 [==============================] - 1s 143us/step - loss: 0.4001 - acc: 0.8342\nEpoch 93/100\n8000/8000 [==============================] - 1s 133us/step - loss: 0.4003 - acc: 0.8342\nEpoch 94/100\n8000/8000 [==============================] - 1s 134us/step - loss: 0.3998 - acc: 0.8354\nEpoch 95/100\n8000/8000 [==============================] - 1s 131us/step - loss: 0.4002 - acc: 0.8341\nEpoch 96/100\n8000/8000 [==============================] - 1s 133us/step - loss: 0.4002 - acc: 0.8365\nEpoch 97/100\n8000/8000 [==============================] - 1s 134us/step - loss: 0.4000 - acc: 0.8345\nEpoch 98/100\n8000/8000 [==============================] - 1s 137us/step - loss: 0.4000 - acc: 0.8351\nEpoch 99/100\n8000/8000 [==============================] - 1s 144us/step - loss: 0.3998 - acc: 0.8352\nEpoch 100/100\n8000/8000 [==============================] - 1s 134us/step - loss: 0.3995 - acc: 0.8356\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "<keras.callbacks.History at 0x7f98c47a3978>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Part 3 - Making the predictions and evaluating the model\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\ny_pred = (y_pred > 0.5)\n",
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "cm",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 18,
          "data": {
            "text/plain": "array([[1547,   48],\n       [ 273,  132]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Merging Test Data and Predicted Test Results. Writing the combined data set into CSV File."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df1 = pd.DataFrame(X_test)\ndf2 = pd.DataFrame(y_pred)\nsubmission = pd.concat([df1,df2],axis = 1)\nsubmission.to_csv('submission.csv', index=False)\nsubmission",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.754865</td>\n      <td>-0.573694</td>\n      <td>-0.552043</td>\n      <td>-1.091687</td>\n      <td>-0.368904</td>\n      <td>1.044737</td>\n      <td>0.879303</td>\n      <td>-0.921591</td>\n      <td>0.642595</td>\n      <td>0.968738</td>\n      <td>1.610857</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.569844</td>\n      <td>-0.573694</td>\n      <td>-1.314903</td>\n      <td>-1.091687</td>\n      <td>0.109617</td>\n      <td>-1.031415</td>\n      <td>0.429722</td>\n      <td>-0.921591</td>\n      <td>0.642595</td>\n      <td>-1.032270</td>\n      <td>0.495870</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.569844</td>\n      <td>1.743090</td>\n      <td>0.571630</td>\n      <td>-1.091687</td>\n      <td>0.301026</td>\n      <td>1.044737</td>\n      <td>0.308583</td>\n      <td>-0.921591</td>\n      <td>0.642595</td>\n      <td>0.968738</td>\n      <td>-0.424787</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.569844</td>\n      <td>-0.573694</td>\n      <td>1.416961</td>\n      <td>0.916013</td>\n      <td>-0.656016</td>\n      <td>-0.339364</td>\n      <td>0.575336</td>\n      <td>-0.921591</td>\n      <td>-1.556190</td>\n      <td>-1.032270</td>\n      <td>-0.187777</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.754865</td>\n      <td>-0.573694</td>\n      <td>0.571630</td>\n      <td>0.916013</td>\n      <td>-0.081791</td>\n      <td>0.006661</td>\n      <td>1.389611</td>\n      <td>0.809503</td>\n      <td>0.642595</td>\n      <td>0.968738</td>\n      <td>0.616842</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-0.569844</td>\n      <td>1.743090</td>\n      <td>0.200509</td>\n      <td>-1.091687</td>\n      <td>1.736588</td>\n      <td>-0.685390</td>\n      <td>1.590021</td>\n      <td>0.809503</td>\n      <td>0.642595</td>\n      <td>-1.032270</td>\n      <td>-0.019302</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-0.569844</td>\n      <td>1.743090</td>\n      <td>-0.624205</td>\n      <td>0.916013</td>\n      <td>-0.464608</td>\n      <td>-1.723466</td>\n      <td>-0.164023</td>\n      <td>0.809503</td>\n      <td>-1.556190</td>\n      <td>0.968738</td>\n      <td>1.045871</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-0.569844</td>\n      <td>1.743090</td>\n      <td>-0.149995</td>\n      <td>-1.091687</td>\n      <td>-0.943129</td>\n      <td>0.352686</td>\n      <td>1.302385</td>\n      <td>0.809503</td>\n      <td>0.642595</td>\n      <td>0.968738</td>\n      <td>0.016166</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>-0.569844</td>\n      <td>-0.573694</td>\n      <td>-0.541734</td>\n      <td>-1.091687</td>\n      <td>2.406518</td>\n      <td>1.390762</td>\n      <td>-1.215717</td>\n      <td>-0.921591</td>\n      <td>-1.556190</td>\n      <td>0.968738</td>\n      <td>-1.511970</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>-0.569844</td>\n      <td>-0.573694</td>\n      <td>-2.005601</td>\n      <td>-1.091687</td>\n      <td>2.310814</td>\n      <td>-1.377440</td>\n      <td>1.426618</td>\n      <td>0.809503</td>\n      <td>-1.556190</td>\n      <td>-1.032270</td>\n      <td>0.705412</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>-0.569844</td>\n      <td>-0.573694</td>\n      <td>-1.572626</td>\n      <td>-1.091687</td>\n      <td>-0.751721</td>\n      <td>1.736788</td>\n      <td>-1.215717</td>\n      <td>0.809503</td>\n      <td>0.642595</td>\n      <td>-1.032270</td>\n      <td>-1.499608</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>-0.569844</td>\n      <td>1.743090</td>\n      <td>0.654101</td>\n      <td>0.916013</td>\n      <td>0.588138</td>\n      <td>1.044737</td>\n      <td>1.195699</td>\n      <td>0.809503</td>\n      <td>-1.556190</td>\n      <td>0.968738</td>\n      <td>0.690310</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1.754865</td>\n      <td>-0.573694</td>\n      <td>-1.675715</td>\n      <td>-1.091687</td>\n      <td>-0.560312</td>\n      <td>-0.339364</td>\n      <td>1.021548</td>\n      <td>-0.921591</td>\n      <td>0.642595</td>\n      <td>-1.032270</td>\n      <td>0.407432</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1.754865</td>\n      <td>-0.573694</td>\n      <td>-0.912855</td>\n      <td>-1.091687</td>\n      <td>-0.751721</td>\n      <td>1.390762</td>\n      <td>0.656410</td>\n      <td>-0.921591</td>\n      <td>0.642595</td>\n      <td>0.968738</td>\n      <td>0.147784</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1.754865</td>\n      <td>-0.573694</td>\n      <td>1.252019</td>\n      <td>-1.091687</td>\n      <td>1.162363</td>\n      <td>1.390762</td>\n      <td>1.084327</td>\n      <td>-0.921591</td>\n      <td>-1.556190</td>\n      <td>0.968738</td>\n      <td>-0.929815</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>1.754865</td>\n      <td>-0.573694</td>\n      <td>0.169582</td>\n      <td>0.916013</td>\n      <td>1.545180</td>\n      <td>1.390762</td>\n      <td>1.251520</td>\n      <td>-0.921591</td>\n      <td>0.642595</td>\n      <td>0.968738</td>\n      <td>0.651826</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>-0.569844</td>\n      <td>-0.573694</td>\n      <td>-1.417992</td>\n      <td>-1.091687</td>\n      <td>0.396730</td>\n      <td>1.390762</td>\n      <td>-1.215717</td>\n      <td>0.809503</td>\n      <td>0.642595</td>\n      <td>-1.032270</td>\n      <td>0.909493</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>-0.569844</td>\n      <td>1.743090</td>\n      <td>0.252053</td>\n      <td>0.916013</td>\n      <td>2.597926</td>\n      <td>0.006661</td>\n      <td>0.632464</td>\n      <td>0.809503</td>\n      <td>0.642595</td>\n      <td>0.968738</td>\n      <td>0.552684</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>-0.569844</td>\n      <td>-0.573694</td>\n      <td>-0.294320</td>\n      <td>-1.091687</td>\n      <td>-0.847425</td>\n      <td>-0.339364</td>\n      <td>0.508211</td>\n      <td>-0.921591</td>\n      <td>-1.556190</td>\n      <td>0.968738</td>\n      <td>1.681109</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>-0.569844</td>\n      <td>-0.573694</td>\n      <td>-1.026253</td>\n      <td>-1.091687</td>\n      <td>0.588138</td>\n      <td>0.352686</td>\n      <td>-1.215717</td>\n      <td>0.809503</td>\n      <td>0.642595</td>\n      <td>0.968738</td>\n      <td>-0.852160</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>1.754865</td>\n      <td>-0.573694</td>\n      <td>0.128346</td>\n      <td>0.916013</td>\n      <td>0.301026</td>\n      <td>0.698712</td>\n      <td>0.636881</td>\n      <td>-0.921591</td>\n      <td>0.642595</td>\n      <td>-1.032270</td>\n      <td>-1.395841</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>-0.569844</td>\n      <td>-0.573694</td>\n      <td>2.056114</td>\n      <td>0.916013</td>\n      <td>-0.368904</td>\n      <td>-1.031415</td>\n      <td>-1.215717</td>\n      <td>0.809503</td>\n      <td>0.642595</td>\n      <td>0.968738</td>\n      <td>-0.750513</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>-0.569844</td>\n      <td>1.743090</td>\n      <td>-1.015944</td>\n      <td>-1.091687</td>\n      <td>-0.464608</td>\n      <td>-0.339364</td>\n      <td>-1.215717</td>\n      <td>0.809503</td>\n      <td>0.642595</td>\n      <td>-1.032270</td>\n      <td>0.697220</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>-0.569844</td>\n      <td>-0.573694</td>\n      <td>-2.129308</td>\n      <td>0.916013</td>\n      <td>-0.464608</td>\n      <td>-1.031415</td>\n      <td>1.090528</td>\n      <td>-0.921591</td>\n      <td>0.642595</td>\n      <td>-1.032270</td>\n      <td>0.217186</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>-0.569844</td>\n      <td>-0.573694</td>\n      <td>1.478815</td>\n      <td>0.916013</td>\n      <td>-0.368904</td>\n      <td>0.352686</td>\n      <td>-1.215717</td>\n      <td>0.809503</td>\n      <td>0.642595</td>\n      <td>0.968738</td>\n      <td>-0.546475</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>-0.569844</td>\n      <td>1.743090</td>\n      <td>1.819009</td>\n      <td>0.916013</td>\n      <td>0.683842</td>\n      <td>-1.377440</td>\n      <td>1.713074</td>\n      <td>-0.921591</td>\n      <td>0.642595</td>\n      <td>0.968738</td>\n      <td>-1.507122</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>-0.569844</td>\n      <td>1.743090</td>\n      <td>-0.624205</td>\n      <td>-1.091687</td>\n      <td>-0.943129</td>\n      <td>-1.031415</td>\n      <td>1.451869</td>\n      <td>0.809503</td>\n      <td>0.642595</td>\n      <td>-1.032270</td>\n      <td>0.387860</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>1.754865</td>\n      <td>-0.573694</td>\n      <td>-1.634480</td>\n      <td>-1.091687</td>\n      <td>-0.847425</td>\n      <td>1.736788</td>\n      <td>0.017456</td>\n      <td>0.809503</td>\n      <td>-1.556190</td>\n      <td>0.968738</td>\n      <td>0.808692</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>-0.569844</td>\n      <td>-0.573694</td>\n      <td>-1.572626</td>\n      <td>-1.091687</td>\n      <td>-0.943129</td>\n      <td>1.390762</td>\n      <td>-1.215717</td>\n      <td>-0.921591</td>\n      <td>0.642595</td>\n      <td>-1.032270</td>\n      <td>1.561898</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>-0.569844</td>\n      <td>-0.573694</td>\n      <td>1.097385</td>\n      <td>-1.091687</td>\n      <td>0.492434</td>\n      <td>-0.339364</td>\n      <td>0.754995</td>\n      <td>-0.921591</td>\n      <td>0.642595</td>\n      <td>-1.032270</td>\n      <td>0.642463</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1970</th>\n      <td>-0.569844</td>\n      <td>1.743090</td>\n      <td>-0.108759</td>\n      <td>0.916013</td>\n      <td>0.779547</td>\n      <td>0.352686</td>\n      <td>0.207273</td>\n      <td>-0.921591</td>\n      <td>0.642595</td>\n      <td>-1.032270</td>\n      <td>0.280073</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1971</th>\n      <td>-0.569844</td>\n      <td>1.743090</td>\n      <td>-0.273702</td>\n      <td>0.916013</td>\n      <td>0.683842</td>\n      <td>-0.685390</td>\n      <td>-1.215717</td>\n      <td>0.809503</td>\n      <td>0.642595</td>\n      <td>0.968738</td>\n      <td>-0.649124</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1972</th>\n      <td>1.754865</td>\n      <td>-0.573694</td>\n      <td>0.097419</td>\n      <td>0.916013</td>\n      <td>-1.038833</td>\n      <td>-1.377440</td>\n      <td>0.676374</td>\n      <td>0.809503</td>\n      <td>0.642595</td>\n      <td>-1.032270</td>\n      <td>-1.492719</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1973</th>\n      <td>-0.569844</td>\n      <td>1.743090</td>\n      <td>0.726264</td>\n      <td>0.916013</td>\n      <td>-0.081791</td>\n      <td>0.698712</td>\n      <td>-1.215717</td>\n      <td>-0.921591</td>\n      <td>-1.556190</td>\n      <td>0.968738</td>\n      <td>-0.810596</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1974</th>\n      <td>-0.569844</td>\n      <td>-0.573694</td>\n      <td>1.571595</td>\n      <td>0.916013</td>\n      <td>-0.560312</td>\n      <td>0.352686</td>\n      <td>-1.215717</td>\n      <td>0.809503</td>\n      <td>0.642595</td>\n      <td>-1.032270</td>\n      <td>0.269477</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1975</th>\n      <td>-0.569844</td>\n      <td>-0.573694</td>\n      <td>-1.108725</td>\n      <td>-1.091687</td>\n      <td>3.076447</td>\n      <td>-1.377440</td>\n      <td>0.451157</td>\n      <td>-0.921591</td>\n      <td>0.642595</td>\n      <td>0.968738</td>\n      <td>-1.295254</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1976</th>\n      <td>-0.569844</td>\n      <td>1.743090</td>\n      <td>0.633483</td>\n      <td>0.916013</td>\n      <td>0.779547</td>\n      <td>-1.377440</td>\n      <td>1.019704</td>\n      <td>-0.921591</td>\n      <td>0.642595</td>\n      <td>0.968738</td>\n      <td>-0.077389</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1977</th>\n      <td>-0.569844</td>\n      <td>-0.573694</td>\n      <td>0.654101</td>\n      <td>0.916013</td>\n      <td>-0.464608</td>\n      <td>0.006661</td>\n      <td>1.040255</td>\n      <td>-0.921591</td>\n      <td>-1.556190</td>\n      <td>0.968738</td>\n      <td>-0.022182</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1978</th>\n      <td>-0.569844</td>\n      <td>-0.573694</td>\n      <td>0.468540</td>\n      <td>0.916013</td>\n      <td>-1.134537</td>\n      <td>-0.339364</td>\n      <td>0.184743</td>\n      <td>0.809503</td>\n      <td>-1.556190</td>\n      <td>-1.032270</td>\n      <td>1.667495</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1979</th>\n      <td>-0.569844</td>\n      <td>-0.573694</td>\n      <td>0.664410</td>\n      <td>0.916013</td>\n      <td>-1.038833</td>\n      <td>0.698712</td>\n      <td>1.347132</td>\n      <td>-0.921591</td>\n      <td>-1.556190</td>\n      <td>-1.032270</td>\n      <td>1.680388</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1980</th>\n      <td>-0.569844</td>\n      <td>1.743090</td>\n      <td>-1.263358</td>\n      <td>0.916013</td>\n      <td>0.396730</td>\n      <td>0.698712</td>\n      <td>0.341934</td>\n      <td>0.809503</td>\n      <td>0.642595</td>\n      <td>0.968738</td>\n      <td>1.036779</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1981</th>\n      <td>-0.569844</td>\n      <td>-0.573694</td>\n      <td>-1.191196</td>\n      <td>-1.091687</td>\n      <td>0.970955</td>\n      <td>-0.685390</td>\n      <td>-1.215717</td>\n      <td>-0.921591</td>\n      <td>-1.556190</td>\n      <td>-1.032270</td>\n      <td>-0.666585</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1982</th>\n      <td>-0.569844</td>\n      <td>-0.573694</td>\n      <td>-1.325212</td>\n      <td>0.916013</td>\n      <td>0.205321</td>\n      <td>0.006661</td>\n      <td>1.087791</td>\n      <td>-0.921591</td>\n      <td>0.642595</td>\n      <td>0.968738</td>\n      <td>-1.484012</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1983</th>\n      <td>-0.569844</td>\n      <td>1.743090</td>\n      <td>-1.263358</td>\n      <td>0.916013</td>\n      <td>-1.517354</td>\n      <td>0.698712</td>\n      <td>0.458126</td>\n      <td>-0.921591</td>\n      <td>0.642595</td>\n      <td>-1.032270</td>\n      <td>1.218231</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1984</th>\n      <td>-0.569844</td>\n      <td>-0.573694</td>\n      <td>-0.778839</td>\n      <td>0.916013</td>\n      <td>-0.943129</td>\n      <td>-0.339364</td>\n      <td>0.731044</td>\n      <td>0.809503</td>\n      <td>0.642595</td>\n      <td>0.968738</td>\n      <td>-0.865622</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1985</th>\n      <td>-0.569844</td>\n      <td>1.743090</td>\n      <td>-0.222157</td>\n      <td>0.916013</td>\n      <td>0.205321</td>\n      <td>1.736788</td>\n      <td>1.183685</td>\n      <td>-0.921591</td>\n      <td>-1.556190</td>\n      <td>-1.032270</td>\n      <td>-1.620514</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1986</th>\n      <td>-0.569844</td>\n      <td>1.743090</td>\n      <td>-1.294285</td>\n      <td>-1.091687</td>\n      <td>-1.325946</td>\n      <td>1.736788</td>\n      <td>-1.215717</td>\n      <td>0.809503</td>\n      <td>0.642595</td>\n      <td>-1.032270</td>\n      <td>-0.535507</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1987</th>\n      <td>-0.569844</td>\n      <td>-0.573694</td>\n      <td>-1.438610</td>\n      <td>0.916013</td>\n      <td>-1.134537</td>\n      <td>1.044737</td>\n      <td>-1.215717</td>\n      <td>0.809503</td>\n      <td>0.642595</td>\n      <td>0.968738</td>\n      <td>-0.887861</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1988</th>\n      <td>-0.569844</td>\n      <td>-0.573694</td>\n      <td>0.499467</td>\n      <td>0.916013</td>\n      <td>0.492434</td>\n      <td>1.044737</td>\n      <td>1.320302</td>\n      <td>-0.921591</td>\n      <td>0.642595</td>\n      <td>-1.032270</td>\n      <td>0.121821</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1989</th>\n      <td>-0.569844</td>\n      <td>1.743090</td>\n      <td>-0.696368</td>\n      <td>-1.091687</td>\n      <td>0.109617</td>\n      <td>-0.685390</td>\n      <td>-0.345941</td>\n      <td>-0.921591</td>\n      <td>0.642595</td>\n      <td>-1.032270</td>\n      <td>0.164535</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1990</th>\n      <td>-0.569844</td>\n      <td>1.743090</td>\n      <td>-1.613862</td>\n      <td>0.916013</td>\n      <td>2.693630</td>\n      <td>0.006661</td>\n      <td>-1.215717</td>\n      <td>0.809503</td>\n      <td>0.642595</td>\n      <td>0.968738</td>\n      <td>-0.248234</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1991</th>\n      <td>-0.569844</td>\n      <td>-0.573694</td>\n      <td>0.953060</td>\n      <td>-1.091687</td>\n      <td>-0.847425</td>\n      <td>-1.377440</td>\n      <td>0.814141</td>\n      <td>-0.921591</td>\n      <td>0.642595</td>\n      <td>0.968738</td>\n      <td>0.671048</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1992</th>\n      <td>-0.569844</td>\n      <td>-0.573694</td>\n      <td>-0.593278</td>\n      <td>0.916013</td>\n      <td>-0.273200</td>\n      <td>-1.031415</td>\n      <td>-0.094204</td>\n      <td>0.809503</td>\n      <td>0.642595</td>\n      <td>-1.032270</td>\n      <td>-0.340040</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1993</th>\n      <td>-0.569844</td>\n      <td>-0.573694</td>\n      <td>-0.407718</td>\n      <td>0.916013</td>\n      <td>-1.038833</td>\n      <td>-1.031415</td>\n      <td>-1.215717</td>\n      <td>0.809503</td>\n      <td>-1.556190</td>\n      <td>-1.032270</td>\n      <td>-1.299673</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1994</th>\n      <td>-0.569844</td>\n      <td>-0.573694</td>\n      <td>-2.180852</td>\n      <td>0.916013</td>\n      <td>-1.038833</td>\n      <td>0.698712</td>\n      <td>0.557704</td>\n      <td>0.809503</td>\n      <td>0.642595</td>\n      <td>-1.032270</td>\n      <td>0.666634</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1995</th>\n      <td>-0.569844</td>\n      <td>-0.573694</td>\n      <td>-0.263393</td>\n      <td>-1.091687</td>\n      <td>-1.421650</td>\n      <td>-1.377440</td>\n      <td>-1.215717</td>\n      <td>0.809503</td>\n      <td>0.642595</td>\n      <td>0.968738</td>\n      <td>1.404319</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1996</th>\n      <td>-0.569844</td>\n      <td>-0.573694</td>\n      <td>-0.665441</td>\n      <td>-1.091687</td>\n      <td>-0.368904</td>\n      <td>0.698712</td>\n      <td>-1.215717</td>\n      <td>0.809503</td>\n      <td>0.642595</td>\n      <td>-1.032270</td>\n      <td>-0.511196</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1997</th>\n      <td>-0.569844</td>\n      <td>1.743090</td>\n      <td>-0.747912</td>\n      <td>0.916013</td>\n      <td>-0.273200</td>\n      <td>-1.377440</td>\n      <td>1.297455</td>\n      <td>0.809503</td>\n      <td>0.642595</td>\n      <td>-1.032270</td>\n      <td>0.718885</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1998</th>\n      <td>1.754865</td>\n      <td>-0.573694</td>\n      <td>-0.005670</td>\n      <td>0.916013</td>\n      <td>-0.464608</td>\n      <td>-0.339364</td>\n      <td>1.059752</td>\n      <td>-0.921591</td>\n      <td>0.642595</td>\n      <td>0.968738</td>\n      <td>-1.545078</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1999</th>\n      <td>1.754865</td>\n      <td>-0.573694</td>\n      <td>-0.799457</td>\n      <td>0.916013</td>\n      <td>-0.847425</td>\n      <td>1.044737</td>\n      <td>0.820263</td>\n      <td>-0.921591</td>\n      <td>0.642595</td>\n      <td>-1.032270</td>\n      <td>1.612559</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>2000 rows × 12 columns</p>\n</div>",
            "text/plain": "            0         1         2         3         4         5         6   \\\n0     1.754865 -0.573694 -0.552043 -1.091687 -0.368904  1.044737  0.879303   \n1    -0.569844 -0.573694 -1.314903 -1.091687  0.109617 -1.031415  0.429722   \n2    -0.569844  1.743090  0.571630 -1.091687  0.301026  1.044737  0.308583   \n3    -0.569844 -0.573694  1.416961  0.916013 -0.656016 -0.339364  0.575336   \n4     1.754865 -0.573694  0.571630  0.916013 -0.081791  0.006661  1.389611   \n5    -0.569844  1.743090  0.200509 -1.091687  1.736588 -0.685390  1.590021   \n6    -0.569844  1.743090 -0.624205  0.916013 -0.464608 -1.723466 -0.164023   \n7    -0.569844  1.743090 -0.149995 -1.091687 -0.943129  0.352686  1.302385   \n8    -0.569844 -0.573694 -0.541734 -1.091687  2.406518  1.390762 -1.215717   \n9    -0.569844 -0.573694 -2.005601 -1.091687  2.310814 -1.377440  1.426618   \n10   -0.569844 -0.573694 -1.572626 -1.091687 -0.751721  1.736788 -1.215717   \n11   -0.569844  1.743090  0.654101  0.916013  0.588138  1.044737  1.195699   \n12    1.754865 -0.573694 -1.675715 -1.091687 -0.560312 -0.339364  1.021548   \n13    1.754865 -0.573694 -0.912855 -1.091687 -0.751721  1.390762  0.656410   \n14    1.754865 -0.573694  1.252019 -1.091687  1.162363  1.390762  1.084327   \n15    1.754865 -0.573694  0.169582  0.916013  1.545180  1.390762  1.251520   \n16   -0.569844 -0.573694 -1.417992 -1.091687  0.396730  1.390762 -1.215717   \n17   -0.569844  1.743090  0.252053  0.916013  2.597926  0.006661  0.632464   \n18   -0.569844 -0.573694 -0.294320 -1.091687 -0.847425 -0.339364  0.508211   \n19   -0.569844 -0.573694 -1.026253 -1.091687  0.588138  0.352686 -1.215717   \n20    1.754865 -0.573694  0.128346  0.916013  0.301026  0.698712  0.636881   \n21   -0.569844 -0.573694  2.056114  0.916013 -0.368904 -1.031415 -1.215717   \n22   -0.569844  1.743090 -1.015944 -1.091687 -0.464608 -0.339364 -1.215717   \n23   -0.569844 -0.573694 -2.129308  0.916013 -0.464608 -1.031415  1.090528   \n24   -0.569844 -0.573694  1.478815  0.916013 -0.368904  0.352686 -1.215717   \n25   -0.569844  1.743090  1.819009  0.916013  0.683842 -1.377440  1.713074   \n26   -0.569844  1.743090 -0.624205 -1.091687 -0.943129 -1.031415  1.451869   \n27    1.754865 -0.573694 -1.634480 -1.091687 -0.847425  1.736788  0.017456   \n28   -0.569844 -0.573694 -1.572626 -1.091687 -0.943129  1.390762 -1.215717   \n29   -0.569844 -0.573694  1.097385 -1.091687  0.492434 -0.339364  0.754995   \n...        ...       ...       ...       ...       ...       ...       ...   \n1970 -0.569844  1.743090 -0.108759  0.916013  0.779547  0.352686  0.207273   \n1971 -0.569844  1.743090 -0.273702  0.916013  0.683842 -0.685390 -1.215717   \n1972  1.754865 -0.573694  0.097419  0.916013 -1.038833 -1.377440  0.676374   \n1973 -0.569844  1.743090  0.726264  0.916013 -0.081791  0.698712 -1.215717   \n1974 -0.569844 -0.573694  1.571595  0.916013 -0.560312  0.352686 -1.215717   \n1975 -0.569844 -0.573694 -1.108725 -1.091687  3.076447 -1.377440  0.451157   \n1976 -0.569844  1.743090  0.633483  0.916013  0.779547 -1.377440  1.019704   \n1977 -0.569844 -0.573694  0.654101  0.916013 -0.464608  0.006661  1.040255   \n1978 -0.569844 -0.573694  0.468540  0.916013 -1.134537 -0.339364  0.184743   \n1979 -0.569844 -0.573694  0.664410  0.916013 -1.038833  0.698712  1.347132   \n1980 -0.569844  1.743090 -1.263358  0.916013  0.396730  0.698712  0.341934   \n1981 -0.569844 -0.573694 -1.191196 -1.091687  0.970955 -0.685390 -1.215717   \n1982 -0.569844 -0.573694 -1.325212  0.916013  0.205321  0.006661  1.087791   \n1983 -0.569844  1.743090 -1.263358  0.916013 -1.517354  0.698712  0.458126   \n1984 -0.569844 -0.573694 -0.778839  0.916013 -0.943129 -0.339364  0.731044   \n1985 -0.569844  1.743090 -0.222157  0.916013  0.205321  1.736788  1.183685   \n1986 -0.569844  1.743090 -1.294285 -1.091687 -1.325946  1.736788 -1.215717   \n1987 -0.569844 -0.573694 -1.438610  0.916013 -1.134537  1.044737 -1.215717   \n1988 -0.569844 -0.573694  0.499467  0.916013  0.492434  1.044737  1.320302   \n1989 -0.569844  1.743090 -0.696368 -1.091687  0.109617 -0.685390 -0.345941   \n1990 -0.569844  1.743090 -1.613862  0.916013  2.693630  0.006661 -1.215717   \n1991 -0.569844 -0.573694  0.953060 -1.091687 -0.847425 -1.377440  0.814141   \n1992 -0.569844 -0.573694 -0.593278  0.916013 -0.273200 -1.031415 -0.094204   \n1993 -0.569844 -0.573694 -0.407718  0.916013 -1.038833 -1.031415 -1.215717   \n1994 -0.569844 -0.573694 -2.180852  0.916013 -1.038833  0.698712  0.557704   \n1995 -0.569844 -0.573694 -0.263393 -1.091687 -1.421650 -1.377440 -1.215717   \n1996 -0.569844 -0.573694 -0.665441 -1.091687 -0.368904  0.698712 -1.215717   \n1997 -0.569844  1.743090 -0.747912  0.916013 -0.273200 -1.377440  1.297455   \n1998  1.754865 -0.573694 -0.005670  0.916013 -0.464608 -0.339364  1.059752   \n1999  1.754865 -0.573694 -0.799457  0.916013 -0.847425  1.044737  0.820263   \n\n            7         8         9         10     0   \n0    -0.921591  0.642595  0.968738  1.610857  False  \n1    -0.921591  0.642595 -1.032270  0.495870  False  \n2    -0.921591  0.642595  0.968738 -0.424787  False  \n3    -0.921591 -1.556190 -1.032270 -0.187777  False  \n4     0.809503  0.642595  0.968738  0.616842  False  \n5     0.809503  0.642595 -1.032270 -0.019302   True  \n6     0.809503 -1.556190  0.968738  1.045871  False  \n7     0.809503  0.642595  0.968738  0.016166  False  \n8    -0.921591 -1.556190  0.968738 -1.511970  False  \n9     0.809503 -1.556190 -1.032270  0.705412   True  \n10    0.809503  0.642595 -1.032270 -1.499608  False  \n11    0.809503 -1.556190  0.968738  0.690310  False  \n12   -0.921591  0.642595 -1.032270  0.407432  False  \n13   -0.921591  0.642595  0.968738  0.147784  False  \n14   -0.921591 -1.556190  0.968738 -0.929815   True  \n15   -0.921591  0.642595  0.968738  0.651826  False  \n16    0.809503  0.642595 -1.032270  0.909493  False  \n17    0.809503  0.642595  0.968738  0.552684  False  \n18   -0.921591 -1.556190  0.968738  1.681109  False  \n19    0.809503  0.642595  0.968738 -0.852160  False  \n20   -0.921591  0.642595 -1.032270 -1.395841  False  \n21    0.809503  0.642595  0.968738 -0.750513  False  \n22    0.809503  0.642595 -1.032270  0.697220  False  \n23   -0.921591  0.642595 -1.032270  0.217186  False  \n24    0.809503  0.642595  0.968738 -0.546475  False  \n25   -0.921591  0.642595  0.968738 -1.507122  False  \n26    0.809503  0.642595 -1.032270  0.387860  False  \n27    0.809503 -1.556190  0.968738  0.808692  False  \n28   -0.921591  0.642595 -1.032270  1.561898  False  \n29   -0.921591  0.642595 -1.032270  0.642463  False  \n...        ...       ...       ...       ...    ...  \n1970 -0.921591  0.642595 -1.032270  0.280073  False  \n1971  0.809503  0.642595  0.968738 -0.649124  False  \n1972  0.809503  0.642595 -1.032270 -1.492719  False  \n1973 -0.921591 -1.556190  0.968738 -0.810596  False  \n1974  0.809503  0.642595 -1.032270  0.269477  False  \n1975 -0.921591  0.642595  0.968738 -1.295254  False  \n1976 -0.921591  0.642595  0.968738 -0.077389  False  \n1977 -0.921591 -1.556190  0.968738 -0.022182  False  \n1978  0.809503 -1.556190 -1.032270  1.667495  False  \n1979 -0.921591 -1.556190 -1.032270  1.680388  False  \n1980  0.809503  0.642595  0.968738  1.036779  False  \n1981 -0.921591 -1.556190 -1.032270 -0.666585   True  \n1982 -0.921591  0.642595  0.968738 -1.484012  False  \n1983 -0.921591  0.642595 -1.032270  1.218231  False  \n1984  0.809503  0.642595  0.968738 -0.865622  False  \n1985 -0.921591 -1.556190 -1.032270 -1.620514  False  \n1986  0.809503  0.642595 -1.032270 -0.535507  False  \n1987  0.809503  0.642595  0.968738 -0.887861  False  \n1988 -0.921591  0.642595 -1.032270  0.121821  False  \n1989 -0.921591  0.642595 -1.032270  0.164535  False  \n1990  0.809503  0.642595  0.968738 -0.248234  False  \n1991 -0.921591  0.642595  0.968738  0.671048  False  \n1992  0.809503  0.642595 -1.032270 -0.340040  False  \n1993  0.809503 -1.556190 -1.032270 -1.299673  False  \n1994  0.809503  0.642595 -1.032270  0.666634  False  \n1995  0.809503  0.642595  0.968738  1.404319  False  \n1996  0.809503  0.642595 -1.032270 -0.511196  False  \n1997  0.809503  0.642595 -1.032270  0.718885  False  \n1998 -0.921591  0.642595  0.968738 -1.545078  False  \n1999 -0.921591  0.642595 -1.032270  1.612559  False  \n\n[2000 rows x 12 columns]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}